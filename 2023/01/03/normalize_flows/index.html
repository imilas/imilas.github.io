<!DOCTYPE html>
<html lang=en>
<head>
    <!-- so meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5" />
    <meta name="description" content="This post is meant as an exercise in implementing a generative normalizing flows model in a 2D environment. It will assume prior knowledge of Jax&#x2F;Flax and normalizing flows.  To familiarize yours">
<meta property="og:type" content="article">
<meta property="og:title" content="Hello">
<meta property="og:url" content="http://imilas.github.io/2023/01/03/normalize_flows/index.html">
<meta property="og:site_name" content="Hello">
<meta property="og:description" content="This post is meant as an exercise in implementing a generative normalizing flows model in a 2D environment. It will assume prior knowledge of Jax&#x2F;Flax and normalizing flows.  To familiarize yours">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://raw.githubusercontent.com/imilas/normalizing-flows-jax-tutorial/main/images/pdf_x.png">
<meta property="og:image" content="https://raw.githubusercontent.com/imilas/normalizing-flows-jax-tutorial/main/images/pdf_y.png">
<meta property="og:image" content="https://raw.githubusercontent.com/imilas/normalizing-flows-jax-tutorial/main/images/target_dist.png">
<meta property="og:image" content="https://raw.githubusercontent.com/imilas/normalizing-flows-jax-tutorial/main/images/normal_dist.png">
<meta property="og:image" content="https://raw.githubusercontent.com/imilas/normalizing-flows-jax-tutorial/main/images/relu_effect.png">
<meta property="og:image" content="https://raw.githubusercontent.com/imilas/normalizing-flows-jax-tutorial/main/images/anim.gif">
<meta property="article:published_time" content="2023-01-04T04:17:26.528Z">
<meta property="article:modified_time" content="2023-01-10T21:15:15.984Z">
<meta property="article:author" content="Amir Salimi">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://raw.githubusercontent.com/imilas/normalizing-flows-jax-tutorial/main/images/pdf_x.png">
    
    
      
        
          <link rel="shortcut icon" href="/images/favicon.ico">
        
      
      
        
          <link rel="icon" type="image/png" href="/images/favicon-192x192.png" sizes="192x192">
        
      
      
        
          <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
        
      
    
    <!-- title -->
    <title>Hello</title>
    <!-- styles -->
    
<link rel="stylesheet" href="/css/style.css">

    <!-- persian styles -->
    
    <!-- rss -->
    
    
	<!-- mathjax -->
	
<meta name="generator" content="Hexo 6.3.0"></head>

<body class="max-width mx-auto px3 ltr">
    
      <div id="header-post">
  <a id="menu-icon" href="#" aria-label="Menu"><i class="fas fa-bars fa-lg"></i></a>
  <a id="menu-icon-tablet" href="#" aria-label="Menu"><i class="fas fa-bars fa-lg"></i></a>
  <a id="top-icon-tablet" href="#" aria-label="Top" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" style="display:none;"><i class="fas fa-chevron-up fa-lg"></i></a>
  <span id="menu">
    <span id="nav">
      <ul>
        <!--
       --><li><a href="/">Home</a></li><!--
     --><!--
       --><li><a href="/about/">About</a></li><!--
     --><!--
       --><li><a href="/archives/">Writing</a></li><!--
     -->
      </ul>
    </span>
    <br/>
    <span id="actions">
      <ul>
        
        
        <li><a class="icon" aria-label="Back to top" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up" aria-hidden="true" onmouseover="$('#i-top').toggle();" onmouseout="$('#i-top').toggle();"></i></a></li>
        <li><a class="icon" aria-label="Share post" href="#"><i class="fas fa-share-alt" aria-hidden="true" onmouseover="$('#i-share').toggle();" onmouseout="$('#i-share').toggle();" onclick="$('#share').toggle();return false;"></i></a></li>
      </ul>
      <span id="i-prev" class="info" style="display:none;">Previous post</span>
      <span id="i-next" class="info" style="display:none;">Next post</span>
      <span id="i-top" class="info" style="display:none;">Back to top</span>
      <span id="i-share" class="info" style="display:none;">Share post</span>
    </span>
    <br/>
    <div id="share" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=http://imilas.github.io/2023/01/03/normalize_flows/"><i class="fab fa-facebook " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=http://imilas.github.io/2023/01/03/normalize_flows/&text="><i class="fab fa-twitter " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=http://imilas.github.io/2023/01/03/normalize_flows/&title="><i class="fab fa-linkedin " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=http://imilas.github.io/2023/01/03/normalize_flows/&is_video=false&description="><i class="fab fa-pinterest " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=&body=Check out this article: http://imilas.github.io/2023/01/03/normalize_flows/"><i class="fas fa-envelope " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=http://imilas.github.io/2023/01/03/normalize_flows/&title="><i class="fab fa-get-pocket " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=http://imilas.github.io/2023/01/03/normalize_flows/&title="><i class="fab fa-reddit " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=http://imilas.github.io/2023/01/03/normalize_flows/&title="><i class="fab fa-stumbleupon " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=http://imilas.github.io/2023/01/03/normalize_flows/&title="><i class="fab fa-digg " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=http://imilas.github.io/2023/01/03/normalize_flows/&name=&description="><i class="fab fa-tumblr " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://news.ycombinator.com/submitlink?u=http://imilas.github.io/2023/01/03/normalize_flows/&t="><i class="fab fa-hacker-news " aria-hidden="true"></i></a></li>
</ul>

    </div>
    <div id="toc">
      <ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#Change-of-Variables"><span class="toc-number">1.</span> <span class="toc-text">Change of Variables</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Code-for-1D-Example"><span class="toc-number">1.1.</span> <span class="toc-text">Code for 1D Example:</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Normalizing-Flows-in-2D-with-Jax"><span class="toc-number">1.2.</span> <span class="toc-text">Normalizing Flows in 2D with Jax</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Defining-Functions"><span class="toc-number">2.</span> <span class="toc-text">Defining Functions</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Coupling-Layers"><span class="toc-number">3.</span> <span class="toc-text">Coupling Layers</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#MultiLayer-Network"><span class="toc-number"></span> <span class="toc-text">MultiLayer Network</span></a>
    </div>
  </span>
</div>

    
    <div class="content index py4">
        
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">
  <header>
    
    <h1 class="posttitle" itemprop="name headline">
        
    </h1>



    <div class="meta">
      <span class="author" itemprop="author" itemscope itemtype="http://schema.org/Person">
        <span itemprop="name">Amir Salimi</span>
      </span>
      
    <div class="postdate">
      
        <time datetime="2023-01-04T04:17:26.528Z" itemprop="datePublished">2023-01-03</time>
        
      
    </div>


      

      

    </div>
  </header>
  

  <div class="content" itemprop="articleBody">
    <p>This post is meant as an exercise in implementing a generative normalizing flows model in a 2D environment. It will assume prior knowledge of Jax&#x2F;Flax and normalizing flows.  To familiarize yourself with Jax&#x2F;Flax, I would recommend the <a target="_blank" rel="noopener" href="https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/JAX/tutorial2/Introduction_to_JAX.html">following notebook</a> by University of Amsterdam’s Deep learning course (which also has other resources included). If you are already familiar with numpy and another deep learning library, learning the basics of Jax should take less than a few hours, or you can try learning it as you go.  To get an understanding of normalizing flows (<a target="_blank" rel="noopener" href="https://jmlr.org/papers/volume22/19-1028/19-1028.pdf">without reading a 50+ page paper</a>) you can take a look at one or more of the following links: </p>
<p><a target="_blank" rel="noopener" href="https://lilianweng.github.io/posts/2018-10-13-flow-models/">Lilian Weng’s blog</a>: Good rundown of the theory and some example functions.<br><a target="_blank" rel="noopener" href="https://blog.evjang.com/2018/01/nf1.html">Eric Jang’s blog</a>: Theory and a tensor-flow (vomit emoji) implementation. I found the code hard to follow since a lot of magic happens in tensor-flow. Eric has since made a pure <a target="_blank" rel="noopener" href="https://blog.evjang.com/2019/07/nf-jax.html">Jax tutorial</a> as well.<br><a target="_blank" rel="noopener" href="https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/JAX/tutorial11/NF_image_modeling.html">UVADLC notebook</a>: Helpful, but much of the content focuses on dealing with the specifics of image data processing (mnist) and advanced architectures, before covering the basics adequately.  I’ve simplified their code for parts of this tutorial.<br><a target="_blank" rel="noopener" href="https://pyro.ai/examples/normalizing_flows_i.html">NF using Pyro library</a>: Pyro tutorials are great in that they focus on conveying a fundamental understanding of the topic rather than the specifics of the library, but I think implementing your own solution in a simplified setting is necessary before using any libraries.  </p>
<p>I will briefly go over the basic theory behind normalizing flows as a refresher, but mainly to establish the variable names that  will be used in the code. </p>
<h3 id="Change-of-Variables"><a href="#Change-of-Variables" class="headerlink" title="Change of Variables"></a>Change of Variables</h3><p>To reiterate the example given by Eric Jang, lets say you have a continuous uniform random variable $X$ and its probability density function (PDF) $P_x(x)$ where $x$ is a real valued output of $X$.  Lets say we applied a function $f(g) &#x3D; 2*g + 1$ to the outputs of $X$. The result can be treated  as a new random variable $Y$ with its own PDF function $P_y(y)$. Where $y$ is the output of $Y$, and $y&#x3D;2x+1$.  Change of variables theorem helps define $P_y$. </p>
<p>Let’s say we give $X$ the range of $[0,1]$, we know what the PDF curve for $X$ would look like. </p>
<p>What would this distribution look like for $Y$?<br>Well, it should be clear that given the range of X, $Y$ would have the range $[1,3]$. But $P_y$ is a different story.<br>Intuitively, $P_y$ is uniform and covers the values between 1 and 3. Since the area under the curve should sum up to 1, then the distribution for $Y$ should look like the following:</p>
<p>But can we prove this mathematically? Yes. </p>
<p>$$<br>\begin{gather}<br>\text{This is not a complete proof!}\<br>p_y(y)dy &#x3D; p_x(x)dx \text{ since area under both is 1} \<br>p_y(y) &#x3D; p_x(x) |\frac{dx}{dy}|\<br>p_y(y) &#x3D; p_x(x) |(\frac{dy}{dx})^{-1}|\<br>p_y(y) &#x3D; p_x(x) |(\frac{df(x))}{dx})^{-1}|\<br>p_y(y) &#x3D; p_x(x) |(\frac{2*x + 1}{dx})^{-1}|\<br>p_y(y) &#x3D; p_x(x) |(2)^{-1}|\<br>p_y(y) &#x3D; p_x(x) * 0.5\<br>\end{gather}<br>$$</p>
<p>This is an example in 1D, but change of variables theorem to determine changes in probability distributions is the basis for normalizing flows. The term $|(\frac{df(x))}{dx})^{-1}|$  is really the determinant of the Jacobin of a multi-variable function $f$, which in a 1D environment is equal to the derivative of $f$. If you want to know what a Jacobian is, Khan academy has a <a target="_blank" rel="noopener" href="https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/jacobian/v/jacobian-prerequisite-knowledge">few short videos plus a quiz</a> which I recommend (it should take about an hour at most if you already know how to take simple derivatives). </p>
<h4 id="Code-for-1D-Example"><a href="#Code-for-1D-Example" class="headerlink" title="Code for 1D Example:"></a>Code for 1D Example:</h4><p>Defining the problem setting and solution in code might seem trivial if we know the answers already. But I found it a helpful exercise before expanding  this idea to a 2 or more dimensional setting. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># let&#x27;s say we have a valid probability distribution where x is between [0,1] and p(x) = 1 </span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> scipy.stats <span class="keyword">import</span> rv_continuous <span class="comment"># scipy has a base class for continuous distributions. You do not have to inherit from it.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># function f, and its reverse</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">fn</span>(<span class="params">x, reverse = <span class="literal">False</span></span>):</span><br><span class="line">    <span class="keyword">if</span> reverse:</span><br><span class="line">        <span class="keyword">return</span> x/<span class="number">2</span> - <span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">2</span>*x + <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">x_dist</span>(<span class="title class_ inherited__">rv_continuous</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_pdf</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> np.where((x&gt;<span class="number">0</span>) &amp; (x&lt;<span class="number">1</span>),<span class="number">1</span>,<span class="number">0</span>)</span><br><span class="line">    </span><br><span class="line">px = x_dist()</span><br><span class="line">x = np.linspace(-<span class="number">0.5</span>,<span class="number">1.5</span>,<span class="number">100</span>)</span><br><span class="line">h = plt.fill_between(x,px.pdf(x),color=<span class="string">&quot;blue&quot;</span>)</span><br><span class="line">plt.legend(handles=[h], labels=[<span class="string">&quot;pdf of X&quot;</span>])</span><br></pre></td></tr></table></figure>
<p><img src="https://raw.githubusercontent.com/imilas/normalizing-flows-jax-tutorial/main/images/pdf_x.png" alt="pdf of X"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># now we know that if x is being projected by f(x), then px will be projected by 1/2</span></span><br><span class="line"><span class="comment"># f(x) = y</span></span><br><span class="line"><span class="comment"># pydy = pxdx</span></span><br><span class="line"><span class="comment"># py = px (dx/(dy) = px (dx/(d(f(x))) = px/(f&#x27;) = px/2 = 0.5px</span></span><br><span class="line"><span class="comment"># so Y PDF will look like this: </span></span><br><span class="line">y = fn(x)</span><br><span class="line">py = <span class="number">0.5</span> * px.pdf(x)</span><br><span class="line">h = plt.fill_between(y,py,color = <span class="string">&quot;g&quot;</span>,)</span><br><span class="line">plt.legend(handles=[h], labels=[<span class="string">&quot;pdf of Y&quot;</span>])</span><br></pre></td></tr></table></figure>
<p><img src="https://raw.githubusercontent.com/imilas/normalizing-flows-jax-tutorial/main/images/pdf_y.png" alt="pdf of  Y"></p>
<h4 id="Normalizing-Flows-in-2D-with-Jax"><a href="#Normalizing-Flows-in-2D-with-Jax" class="headerlink" title="Normalizing Flows in 2D with Jax"></a>Normalizing Flows in 2D with Jax</h4><p>Let’s define the same 2D dataset as Eric Jang’s problem. We generate 1024 samples, and each sample has 2 points. There is a relationship between the two points; to create a generative model, we want to find a series of invertible functions which can untangle the “complicated” relationships, and project each point to a unimodal normal distribution. If successful, generating new samples is trivial: we take 2 samples from a gaussian distribution and stack them as $z &#x3D; (z_1, z_2)$ , and send $z$ backwards through the flow. In other words, transform $z$ by  reversing the order of functions <em>and</em> sending it through the inverse of each function. An implication of this is that dimensionality remains the same in normaling flows. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># define a target distribution</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_points</span>():</span><br><span class="line">    x2 = <span class="number">0</span> + <span class="number">4</span> * np.random.normal()</span><br><span class="line">    x1 = <span class="number">0.25</span> * x2**<span class="number">2</span> + np.random.normal()</span><br><span class="line">    <span class="keyword">return</span> x1,x2</span><br><span class="line">x_samples = np.array([get_points() <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1024</span>)])</span><br><span class="line">t_dist = jnp.array(x_samples)</span><br><span class="line">sns.scatterplot(x = x_samples[:,<span class="number">0</span>],y = x_samples[:,<span class="number">1</span>])</span><br></pre></td></tr></table></figure>
<p><img src="https://raw.githubusercontent.com/imilas/normalizing-flows-jax-tutorial/main/images/target_dist.png" alt="pdf of X"></p>
<p>Regardless of your target distribution,  after going through the flow, you want to final distribution to be a multi-variable normal distribution with no covariance&#x2F;correlation.  We assume the marginal distribution of each point after the transformations to be normal, and calculate the loss based on this assumption. This means that during training, the parameters of the functions are adjusted such that the points are projected into a multi-variate normal distribution, so something like this:</p>
<p><img src="https://raw.githubusercontent.com/imilas/normalizing-flows-jax-tutorial/main/images/normal_dist.png" alt="pdf of X"></p>
<p>But how do we calculate this loss values? This is a critical step that did not click with me right away. If my explanation isn’t clear here please read one of the recommended posts mentioned early on. Let’s assume the flow only has 1 function, $f$. </p>
<p>We know that  $p_y(y) &#x3D; p_x(x)  |(\frac{df(x))}{dx})^{-1}|$ , which can be rewritten by replacing $f’$ with the determinant of the Jacobian of  $f$ and taking the log of both sides:<br>$$ \begin{gather}<br>p_y(y) &#x3D; p_x(x)   |\det(J(f))^{-1}|\<br>\log(p_y(y)) &#x3D; log(p_x(x)) - log(|\det(J(f)|)<br>\end{gather}<br>$$<br>If we have k (here, $k&#x3D;1024$) samples of $X$ (i.e, ${x^1, x^2,…,x^k} \in X$  is the input) , and $x^k&#x3D;(x{^k}_1,x{^k}_2)$  and we want to generate new samples that fit $X$, what we do is project each $x$ to a value $y&#x3D;(y_1,y_2)$ using a parameterized function $f$, and calculate the PDF of the two points in $y$ under a normal distribution. From there we can calculate $log(P_x)$ (which you can define as the sum of probabilities for each of the two points) using the formula above. We want to maximize $P_x$, the probability of the samples, therefore $-log(P_x)$ is minimized during training. </p>
<h3 id="Defining-Functions"><a href="#Defining-Functions" class="headerlink" title="Defining Functions"></a>Defining Functions</h3><p>Here we implement leakyRelu and realNVP, two common functions&#x2F;layers used in normalizing flows. When defining layers, we have to define how it transforms an input, and also how to calculate the log determinant of the jacobian (LDJ). LDJs are passed through and added up in every layer, and are needed to calculated the projected probabilities. The probabilities are then used to calculate the loss of the network. </p>
<p>In a LeaklyRelu layer, if an input $x&lt;0$, it is multiplied by a value $\alpha$, else, it is unchanged. The LDJ for each individual point in a sample is:<br>$$</p>
<pre><code>LDJ(z_i) = 
</code></pre>
<p>\begin{cases}<br>    log(|\alpha|) &amp; \text{if } x\le 0\<br>    0              &amp; x\gt 0\<br>\end{cases}</p>
<p>$$<br>Here, $i$ is either 1 or 2 since each sample is in 2D. Since the final goal is to calculate the loss function, you can sum up the LDJ values before passing it on to the subsequent layer. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># using Flax&#x27;s nn module</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LeakyRelu</span>(nn.Module):        </span><br><span class="line">    layer_type = <span class="string">&quot;leaky_relu&quot;</span> <span class="comment"># a name that will be used when plotting intermediate layers</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">setup</span>(<span class="params">self</span>):</span><br><span class="line">        self.alpha = self.param(<span class="string">&#x27;alpha&#x27;</span>, nn.initializers.uniform(<span class="number">0.9</span>), (<span class="number">1</span>,)) <span class="comment"># alpha is randomly sampled from a uniform distribution between 0 and 1</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, z, ldj,rng, reverse=<span class="literal">False</span></span>):</span><br><span class="line">        rng, new_rng = jax.random.split(rng, <span class="number">2</span>)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> reverse:</span><br><span class="line">            dj = jnp.where(z&gt;<span class="number">0</span>, <span class="number">1</span>, self.alpha)</span><br><span class="line">            z = jnp.where(z&gt;<span class="number">0</span>, z, self.alpha * z)</span><br><span class="line">            ldj += jnp.log(jnp.absolute(dj)).<span class="built_in">sum</span>(axis=[<span class="number">1</span>])</span><br><span class="line">            <span class="keyword">return</span> z, ldj, new_rng</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            z = jnp.where(z&gt;<span class="number">0</span>, z, z/self.alpha)</span><br><span class="line">            dj = jnp.where(z&gt;<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>/self.alpha)</span><br><span class="line">            ldj -= jnp.log(jnp.absolute(dj)).<span class="built_in">sum</span>(axis=[<span class="number">1</span>]) </span><br><span class="line">            <span class="keyword">return</span> z, ldj, new_rng</span><br><span class="line">    </span><br></pre></td></tr></table></figure>

<p>You can visualize the effect of the layer here.  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># initialize an lru layer </span></span><br><span class="line">lru = LeakyRelu()</span><br><span class="line">rng, inp_rng, init_rng = jax.random.split(rng, <span class="number">3</span>)</span><br><span class="line">params = lru.init(init_rng,x_samples,<span class="number">0</span>,inp_rng)</span><br><span class="line"></span><br><span class="line"><span class="comment"># go forward and check LDJ</span></span><br><span class="line">z = t_dist</span><br><span class="line">ldj = jnp.zeros(<span class="built_in">len</span>(z))</span><br><span class="line">sns.scatterplot(x= z[:,<span class="number">0</span>], y = z[:,<span class="number">1</span>],label=<span class="string">&quot;original&quot;</span>)</span><br><span class="line">z,ldj,rng = lru.apply(params,z,<span class="number">0</span>,rng,reverse=<span class="literal">False</span>)</span><br><span class="line">sns.scatterplot(x= z[:,<span class="number">0</span>], y = z[:,<span class="number">1</span>],label=<span class="string">&quot;forward&quot;</span>)</span><br></pre></td></tr></table></figure>

<p><img src="https://raw.githubusercontent.com/imilas/normalizing-flows-jax-tutorial/main/images/relu_effect.png" alt="pdf of X"><br>Here inspecting the the <code>params</code> and <code>LDJ</code> values should look something like this:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">FrozenDict(&#123;</span><br><span class="line">    params: &#123;</span><br><span class="line">        alpha: DeviceArray([0.73105997], dtype=float32),</span><br><span class="line">    &#125;,</span><br><span class="line">&#125;)</span><br><span class="line">ldj list: [-0.31325978 -0.31325978  0.         ... -0.31325978 -0.31325978</span><br><span class="line">  0.        ]</span><br></pre></td></tr></table></figure>

<p>where the outputs can take 3 values: 0, $\log{\alpha}$, or $2*\log{\alpha}$ (if both points were less than 0). Here $\log{0.73} \approx -0.313$. </p>
<h3 id="Coupling-Layers"><a href="#Coupling-Layers" class="headerlink" title="Coupling Layers"></a>Coupling Layers</h3><p>A coupling layer is a layer where a subset of points in a sample (which here is either $x_1$ or $x_2$) are unaffected by the transformation. However, this unchanged subset determines the change in the remaining points.  Here $mask$ is the indices that will remain unchanged. These indices can be given to any function (typically a neural network) that returns a scaling parameter $s$ and a transform parameter $t$. $s$ and $t$ are used to scale and shift the unmasked points. </p>
<p>$$</p>
<p>\begin{cases}<br>  y_{\in mask} &#x3D; x_{\in mask}\<br>   y_{\notin mask} &#x3D; x_{\notin mask} \cdot exp(s(x_{\in mask})) + t(x_{\in mask})<br>\end{cases}</p>
<p>$$<br>This setup allows for coupling, or transformation of some points according to the value of other points. But we actually do not need to know the inverse of the neural network, or whatever method we use to calculate $s$ and $t$. The Jacobian is simply a triangular matrix with the diagonal equal to 1 at the masked indices, and equal to $exp(s(x_{\in mask})$ at the unmasked indices. So the LDJ is simply the product the diagonal, or $\prod_{0}^{j}exp(s)$  where j is the number of unmasked indices (which is always 1 here). </p>
<p>Here I’ve modified an example provided by University of Amsterdam deep learning course for the 2D example. A simple neural network (also called a hyper-network) generates the scale and transform parameters for one of the two points based on the value of the other. The coupling layer then transforms the sample and calculates the LDJ. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># modified from https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/JAX/tutorial2/Introduction_to_JAX.html</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleNet</span>(nn.Module):</span><br><span class="line">    num_hidden : <span class="built_in">int</span>   <span class="comment"># Number of hidden neurons</span></span><br><span class="line">    num_outputs : <span class="built_in">int</span>  <span class="comment"># Number of output neurons</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">setup</span>(<span class="params">self</span>):</span><br><span class="line">        self.linear1 = nn.Dense(features=self.num_hidden)</span><br><span class="line">        self.linear2 = nn.Dense(features=self.num_outputs)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># Perform the calculation of the model to determine the prediction</span></span><br><span class="line">        x = self.linear1(x)</span><br><span class="line">        x = nn.tanh(x)</span><br><span class="line">        x = self.linear2(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CouplingLayer</span>(nn.Module):</span><br><span class="line">    network : nn.Module  <span class="comment"># NN to use in the flow for predicting mu and sigma</span></span><br><span class="line">    mask : np.ndarray  <span class="comment"># Binary mask where 0 denotes that the element should be transformed, and 1 not.</span></span><br><span class="line">    c_in : <span class="built_in">int</span>  <span class="comment"># Number of input channels</span></span><br><span class="line">    layer_type = <span class="string">&quot;Scale and Shift&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">setup</span>(<span class="params">self</span>):</span><br><span class="line">        self.scaling_factor = self.param(<span class="string">&#x27;scaling_factor&#x27;</span>,</span><br><span class="line">                                         nn.initializers.zeros,</span><br><span class="line">                                         (self.c_in,))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, z, ldj, rng, reverse=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="comment"># Apply network to masked input</span></span><br><span class="line">        z_in = z * self.mask</span><br><span class="line">        nn_out = self.network(z_in)</span><br><span class="line">        s, t = nn_out.split(<span class="number">2</span>, axis=-<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># Stabilize scaling output</span></span><br><span class="line">        s_fac = jnp.exp(self.scaling_factor).reshape(<span class="number">1</span>, -<span class="number">1</span>)</span><br><span class="line">        s = nn.tanh(s / s_fac) * s_fac</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Mask outputs (only transform the second part)</span></span><br><span class="line">        s = s * (<span class="number">1</span> - self.mask)</span><br><span class="line">        t = t * (<span class="number">1</span> - self.mask)</span><br><span class="line">        <span class="comment"># Affine transformation</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> reverse:</span><br><span class="line">            <span class="comment"># Whether we first shift and then scale, or the other way round,</span></span><br><span class="line">            <span class="comment"># is a design choice, and usually does not have a big impact</span></span><br><span class="line">            z = (z + t) * jnp.exp(s)</span><br><span class="line">            ldj += s.<span class="built_in">sum</span>(axis=[<span class="number">1</span>])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            z = (z * jnp.exp(-s)) - t</span><br><span class="line">            ldj -= s.<span class="built_in">sum</span>(axis=[<span class="number">1</span>])</span><br><span class="line">        <span class="keyword">return</span> z, ldj, rng</span><br></pre></td></tr></table></figure>

<h1 id="MultiLayer-Network"><a href="#MultiLayer-Network" class="headerlink" title="MultiLayer Network"></a>MultiLayer Network</h1><p>Finally, we want to create a model with multiple layers, train it, and sample from it when we are done training.  This is very simple to do with flax.  The sampling function could be even simpler if we didn’t want to see the intermediate transformations. Notice that during sampling, we simply generate two points from a normal distribution and pass it back through the flows. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># multi layer network</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">PointFlow</span>(nn.Module):</span><br><span class="line">    flows : <span class="type">Sequence</span>[nn.Module]  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, z, rng, intermediate=<span class="literal">False</span>,training=<span class="literal">True</span></span>):</span><br><span class="line">        ldj = <span class="number">0</span> </span><br><span class="line">        rng, rng_new = jax.random.split(rng,<span class="number">2</span>)</span><br><span class="line">        <span class="keyword">for</span> flow <span class="keyword">in</span> self.flows:</span><br><span class="line">            z, ldj, rng = flow(z, ldj, rng, reverse=<span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">return</span> z, ldj, rng_new</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">sample</span>(<span class="params">self,rng,num = <span class="number">1</span>, intermediate = <span class="literal">False</span></span>):</span><br><span class="line">        ldj = <span class="number">0</span></span><br><span class="line">        rng, new_rng = jax.random.split(rng,<span class="number">2</span>)</span><br><span class="line">        z = jax.random.normal(rng,shape=[num,<span class="number">2</span>])</span><br><span class="line">        intermediate_layers = [z]</span><br><span class="line">        <span class="keyword">for</span> flow <span class="keyword">in</span> <span class="built_in">reversed</span>(self.flows):</span><br><span class="line">            z, ldj, rng = flow(z,ldj,rng,reverse=<span class="literal">True</span>)</span><br><span class="line">            <span class="keyword">if</span> intermediate: <span class="comment"># if we want to see the intermediate results</span></span><br><span class="line">                intermediate_layers.append(z)</span><br><span class="line">        <span class="keyword">return</span> z, rng, intermediate_layers</span><br></pre></td></tr></table></figure>

<p>The <code>PointsFlow</code> model just needs a list of flow layers, we’ve already defined Relu and Coupling layers. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">flow_layers = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">9</span>):</span><br><span class="line">    flow_layers += [LeakyRelu()]</span><br><span class="line">    flow_layers += [CouplingLayer(network = SimpleNet(num_hidden=<span class="number">4</span>,num_outputs=<span class="number">2</span>),mask = jnp.array([<span class="number">0</span>,<span class="number">1</span>]) <span class="keyword">if</span> i%<span class="number">2</span> <span class="keyword">else</span> jnp.array([<span class="number">1</span>,<span class="number">0</span>]),c_in=<span class="number">1</span>)]</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">model = PointFlow(flow_layers)</span><br><span class="line">params = model.init(init_rng,inp,inp_rng,intermediate=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>


<p>Then we need to define the loss calculation and optimizer. I found that adamw worked much better than SGD and adam. The loss function is where we calculate the negative log loss, which requires and understanding of the change of variable rules. Try to derive it yourself on paper to make sure you understand how this works (Lilian Wang’s blog was very helpful for me in this regard)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">optimizer = optax.adamw(learning_rate=<span class="number">0.0001</span>)</span><br><span class="line">model_state = train_state.TrainState.create(apply_fn=model.apply,</span><br><span class="line">                                            params=params,</span><br><span class="line">                                            tx=optimizer)</span><br><span class="line"><span class="comment"># loss calculation</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">loss_calc</span>(<span class="params">state, params, rng, batch</span>):</span><br><span class="line">    rng, init_rng = jax.random.split(rng,<span class="number">2</span>)</span><br><span class="line">    z, ldj, rng  = state.apply_fn(params,batch,rng)</span><br><span class="line">    <span class="comment"># loss is ldj + log(probability of points in a normal distribution)</span></span><br><span class="line">    log_pz = jax.scipy.stats.norm.logpdf(z).<span class="built_in">sum</span>(axis=[<span class="number">1</span>])</span><br><span class="line">    log_px = ldj + log_pz</span><br><span class="line">    nll = -log_px</span><br><span class="line">    <span class="keyword">return</span> nll.mean(), rng</span><br><span class="line">    </span><br><span class="line">nll, rng = loss_calc(model_state,params,rng,inp) <span class="comment"># run it once and see the output </span></span><br></pre></td></tr></table></figure>

<p>Finally, we define a train_step function. If you have a GPU, the jitted function runs incredibly fast. Just in time compilation is one of the best features of Jax.  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@jax.jit</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_step</span>(<span class="params">state,rng,batch</span>):</span><br><span class="line">    rng, init_rng = jax.random.split(rng,<span class="number">2</span>)</span><br><span class="line">    grad_fn = jax.value_and_grad(loss_calc,  <span class="comment"># Function to calculate the loss</span></span><br><span class="line">                             argnums=<span class="number">1</span>,  <span class="comment"># Parameters are second argument of the function</span></span><br><span class="line">                             has_aux=<span class="literal">True</span>  <span class="comment"># Function has additional outputs, here rng. Which you don&#x27;t even need now that I think about it. </span></span><br><span class="line">                            )</span><br><span class="line">    <span class="comment"># Determine gradients for current model, parameters and batch</span></span><br><span class="line">    (loss,rng), grads = grad_fn(state, state.params, rng, batch)</span><br><span class="line">    <span class="comment"># Perform parameter update with gradients and optimizer</span></span><br><span class="line">    state = state.apply_gradients(grads=grads)</span><br><span class="line">    <span class="comment"># Return state and any other value we might want</span></span><br><span class="line">    <span class="keyword">return</span> state, rng, loss</span><br></pre></td></tr></table></figure>

<p>Training is simple :)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">state = model_state</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">50000</span>):</span><br><span class="line">    state,rng,loss = train_step(state,rng,X)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;iter %d patience %d loss %f&quot;</span>%(i,patience, loss) , end=<span class="string">&quot;\r&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>And so is sampling:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">layers = [<span class="string">&quot;random sample&quot;</span>] + [model.flows[i].layer_type <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(model.flows))][::-<span class="number">1</span>] <span class="comment"># names of generation layers (backward order of training layers)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i,out <span class="keyword">in</span> <span class="built_in">enumerate</span>(mid_layers):</span><br><span class="line">    sns.scatterplot(x= out[:,<span class="number">0</span>], y = out[:,<span class="number">1</span>],label = <span class="string">&quot;out of&quot;</span> + layers[i])</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
<p>It’s easier to show a gif rather than 19 scatter plots:<br><img src="https://raw.githubusercontent.com/imilas/normalizing-flows-jax-tutorial/main/images/anim.gif" alt="transformation animation"></p>

  </div>
</article>



        
          <div id="footer-post-container">
  <div id="footer-post">

    <div id="nav-footer" style="display: none">
      <ul>
         
          <li><a href="/">Home</a></li>
         
          <li><a href="/about/">About</a></li>
         
          <li><a href="/archives/">Writing</a></li>
        
      </ul>
    </div>

    <div id="toc-footer" style="display: none">
      <ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#Change-of-Variables"><span class="toc-number">1.</span> <span class="toc-text">Change of Variables</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Code-for-1D-Example"><span class="toc-number">1.1.</span> <span class="toc-text">Code for 1D Example:</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Normalizing-Flows-in-2D-with-Jax"><span class="toc-number">1.2.</span> <span class="toc-text">Normalizing Flows in 2D with Jax</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Defining-Functions"><span class="toc-number">2.</span> <span class="toc-text">Defining Functions</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Coupling-Layers"><span class="toc-number">3.</span> <span class="toc-text">Coupling Layers</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#MultiLayer-Network"><span class="toc-number"></span> <span class="toc-text">MultiLayer Network</span></a>
    </div>

    <div id="share-footer" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=http://imilas.github.io/2023/01/03/normalize_flows/"><i class="fab fa-facebook fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=http://imilas.github.io/2023/01/03/normalize_flows/&text="><i class="fab fa-twitter fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=http://imilas.github.io/2023/01/03/normalize_flows/&title="><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=http://imilas.github.io/2023/01/03/normalize_flows/&is_video=false&description="><i class="fab fa-pinterest fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=&body=Check out this article: http://imilas.github.io/2023/01/03/normalize_flows/"><i class="fas fa-envelope fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=http://imilas.github.io/2023/01/03/normalize_flows/&title="><i class="fab fa-get-pocket fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=http://imilas.github.io/2023/01/03/normalize_flows/&title="><i class="fab fa-reddit fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=http://imilas.github.io/2023/01/03/normalize_flows/&title="><i class="fab fa-stumbleupon fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=http://imilas.github.io/2023/01/03/normalize_flows/&title="><i class="fab fa-digg fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=http://imilas.github.io/2023/01/03/normalize_flows/&name=&description="><i class="fab fa-tumblr fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://news.ycombinator.com/submitlink?u=http://imilas.github.io/2023/01/03/normalize_flows/&t="><i class="fab fa-hacker-news fa-lg" aria-hidden="true"></i></a></li>
</ul>

    </div>

    <div id="actions-footer">
        <a id="menu" class="icon" href="#" onclick="$('#nav-footer').toggle();return false;"><i class="fas fa-bars fa-lg" aria-hidden="true"></i> Menu</a>
        <a id="toc" class="icon" href="#" onclick="$('#toc-footer').toggle();return false;"><i class="fas fa-list fa-lg" aria-hidden="true"></i> TOC</a>
        <a id="share" class="icon" href="#" onclick="$('#share-footer').toggle();return false;"><i class="fas fa-share-alt fa-lg" aria-hidden="true"></i> Share</a>
        <a id="top" style="display:none" class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up fa-lg" aria-hidden="true"></i> Top</a>
    </div>

  </div>
</div>

        
        <footer id="footer">
  <div class="footer-left">
    Copyright &copy;
    
    
    2023
    Amir Salimi
  </div>
  <div class="footer-right">
    <nav>
      <ul>
        <!--
       --><li><a href="/">Home</a></li><!--
     --><!--
       --><li><a href="/about/">About</a></li><!--
     --><!--
       --><li><a href="/archives/">Writing</a></li><!--
     -->
      </ul>
    </nav>
  </div>
</footer>

    </div>
    <!-- styles -->



  <link rel="preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.2/css/all.min.css" crossorigin="anonymous" onload="this.onload=null;this.rel='stylesheet'"/>


    <!-- jquery -->
 
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js" crossorigin="anonymous"></script> 




<!-- clipboard -->

  
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.7/clipboard.min.js" crossorigin="anonymous"></script> 
  
  <script type="text/javascript">
  $(function() {
    // copy-btn HTML
    var btn = "<span class=\"btn-copy tooltipped tooltipped-sw\" aria-label=\"Copy to clipboard!\">";
    btn += '<i class="far fa-clone"></i>';
    btn += '</span>'; 
    // mount it!
    $(".highlight table").before(btn);
    var clip = new ClipboardJS('.btn-copy', {
      text: function(trigger) {
        return Array.from(trigger.nextElementSibling.querySelectorAll('.code')).reduce((str,it)=>str+it.innerText+'\n','')
      }
    });
    clip.on('success', function(e) {
      e.trigger.setAttribute('aria-label', "Copied!");
      e.clearSelection();
    })
  })
  </script>


<script src="/js/main.js"></script>

<!-- search -->

<!-- Google Analytics -->

<!-- Baidu Analytics -->

<!-- Cloudflare Analytics -->

<!-- Umami Analytics -->

<!-- Disqus Comments -->

<!-- utterances Comments -->

</body>
</html>
